
# [0.Quotes Scraper](https://github.com/ashwinshetgaonkar/Web-Quotes-Scraper)
Web scraping or web data extraction is used for extracting data from websites,this involves fetching data and extracting/parsing required info from it. Fetching is the downloading of a page (which a browser does when a user views a page).

* Implemented Web Scraping to extract data from static sites.
* Used `requests` and `BeautifulSoup` libraries to perform the web scraping.
* The project involved extracting the names of all topics of quotes and then scraping quotes along with their author name for all topics
  previously scraped.<br>
<!-- * Deployed the project using streamlit as a Web app. -->
* Tech stack used:`Python,requests,BeautifulSoup,pandas`.<br>
<!--   [To view the web app](https://share.streamlit.io/ashwinshetgaonkar/web-quotes-scraper/main/app.py) -->

  



# [1. Predict Price of Old Car](https://github.com/karanchinch10/Oldcar_Sell_Regression)                                      
 
Now a day many peoples prefer to <strong>buy second hand car instead of buying new one</strong>, as its better investment option where we get almost <strong>30-40% discount</strong>. but main question here is how seller will know <strong>actual selling price of old car</strong> base on car features or which factors play major roles?? So to solve this complex problem, I have build <strong>ML model</strong> which predict <strong>estimated price of car</strong> base on given input features as <strong>brand,KM drive,Power,Year and so on..

* Completed stepwise <strong>EDA (Exploratory Data Analysis)</strong> and visualization to get data insight & to know <strong>important features also their correlation</strong> with car price
* Done <strong>Feature Engineering</strong> includes <strong>Features extraction & Features construction</strong> based on my domian knowledge and visualization
* <strong>Train model</strong> with multiple regression algorithms then Analysed & compare performance of differents models based of <strong>accuracy and complexity</strong>
* After comparing, got well accuracy by <strong>RandomForestRegressor(cross validation--around 90%)</strong>
* <strong>Build Web App</strong> using streamlit and <strong>deploy</strong> the model 
* Tech stack used:`Python,Numpy,Pandas,sklearn,matplotllib,seaborn,html,css`.<br>
* * [To view on kaggle](https://www.kaggle.com/code/karanchinchpure/predict-price-of-used-cars-regression-problem) üíù
  * [Web App](https://karanchinch10-oldcar-sell-streamlit-app-p6gwqq.streamlitapp.com) üíù
  


# [2. Bank Marketing Campaign](https://github.com/ashwinshetgaonkar/Data-Visualization-Projects/tree/main/Super%20Store%20Analysis)
<strong>Marketing campaigns</strong> are sets of strategic activities that promote a <strong>business‚Äôs goal</strong> or objective. A marketing campaign could be used to promote a product, a service, or the brand as a whole. The following project focus on the analysis of a dataset of <strong>Bank Marketing</strong> which contains data or information about customers and aims to get useful insights from the data and <strong>predict if a new customer will accept a deposit offer or not</strong>.

* Completed stepwise <strong>EDA (Exploratory Data Analysis)</strong> then visualizatiion to get some idea about important features or correlation
* Done <strong>Feature Engineering</strong> which includes features extraction & features construction based on domain knowledge and <strong>visualization</strong> followed by label encoding
* Train ML models with multiples algorithms then Analysed & compare <strong>performance of differents models</strong> based of accuracy and complexity
* after comparing with all, got well accuracy by <strong>RandomForest and XG boost</strong>
* Build <strong>Pipeline</strong> for <strong>deployment</strong> session
*  Tech stack used:`Python,pandas,matplotlib,seaborn,numpy,html,css`.<br>
   [To view on kaggle](https://www.kaggle.com/code/karanchinchpure/bank-marketing-who-will-subscribe-for-deposit) 
   


<!-- # [4.Road Deaths Analysis](https://github.com/ashwinshetgaonkar/Data-Visualization-Projects/tree/main/Road%20Deaths%20Analysis)
* The Dataset contains information of number of deaths in various regions of the World from 1990-2019,along with other data like historical population,region code,Side of driving.

* My objective for this Project was to visualize the available data to draw insights from it which are not perceived just by reading through an excel/csv file.
* Here I have visualized the number of deaths using various plots to gain various insights from the data.
* From this I can easily state the regions with maximum,mean deaths,year in which max deaths occured and many more.<br>
  [To view on kaggle](https://www.kaggle.com/code/ashwinshetgaonkar/road-deaths-data-visualization-seaborn) -->
  


# [4. Visualization of Google Playstore Apps](https://github.com/ashwinshetgaonkar/Estimate-Mechanical-Properties-of-Steel-compostions)
<strong>Google Play Store</strong> team is about to launch a new feature wherein, certain apps that are promising, are boosted in visibility. The boost will manifest in multiple ways including <strong>higher priority in recommendations sections</strong> (‚ÄúSimilar apps‚Äù, ‚ÄúYou might also like‚Äù, ‚ÄúNew and updated games‚Äù). These will also get a boost in search results visibility. This feature will help bring more attention to <strong>newer apps that have the potential.</strong>

* Perform <strong>EDA, Data cleaning and Data correction</strong> 
* Done details <strong>visualization</strong> on gplay store apps to get basic information or data insight and that will be helpful for <strong>decision making</strong> like
* <strong>Total No of apps</strong> of all category (like games,sports,medical,education,beauty..etc) to understand whcih category has <strong>highest apps</strong>
* Which <strong>category</strong> has <strong>highest demand</strong>, rating, installation & reviews
* Total <strong>percentages of free and paid apps</strong> available in glapy store
* Is there is any <strong>relation of apps rating and reviews with insatllation?</strong>
* Tech stack used : `Python, numpy, pandas, matplotlib, seaborn, lightgbm, optuna, streamlit, html, sklearn, scipy, joblib.`<br>
  [To view on kaggle](https://www.kaggle.com/code/ashwinshetgaonkar/mech-prop-lightgbm-optuna),[To view the web app](https://share.streamlit.io/ashwinshetgaonkar/estimate-mechanical-properties-of-steel-compostions/main/app.py)
 

# [5. Bank Management Web Application](https://github.com/ashwinshetgaonkar/Movie-Rating-Sentiment-Analysis)
* I have made this <strong>flask project of bank management web application system</strong> 
* Project is specially designed for <strong>customer/bank holder</strong> to get all <strong>basic bank services</strong>
* First customer has to <strong>open their bank account</strong> by filling basic bank details such as Name, Password, DOB, mob no, Initial Deposit and register their bank account
* Once account has  been created then they can <strong>login</strong> with their <strong>user ID and password</strong>
* User can <strong>view and modify</strong> their <strong>personal details</strong> from profile section
* User can <strong>withdraw, credit money</strong> into their account also can <strong>check current balance</strong>
* Tech stack used:`Python,numpy,pandas,sklearn,tensorflow,streamlit,html,css,tranformers.`<br>
  [To view on kaggle](https://www.kaggle.com/code/ashwinshetgaonkar/movie-rating-sentiment-analysis),[To view the web app](https://share.streamlit.io/ashwinshetgaonkar/movie-rating-sentiment-analysis/main/app.py)
  
  

<!-- # [8.Fake News Classifier](https://github.com/ashwinshetgaonkar/Fake-News-Classifier)
* In today's world which contains a lot of digital data it will be very beneficial to have some kind of an software that will help us in descriminating between Fake and Real News with some given constraints.
* The dataset contains news instances with title and text along with its labels taken from various sources.
* My objective for this project was to train and compare the performance of various models on the basis of f1_score and time taken per prediction.
* Here I have demostrated how increasing the complexity of the model will lead to better performance but will hamper the time taken per prediction.
* Build an web app using streamlit which uses model trained using a feed forward neutral network.<br>
  [To view on kaggle](https://www.kaggle.com/code/ashwinshetgaonkar/fake-news-classifier-nb-bert),[To view the web app](https://share.streamlit.io/ashwinshetgaonkar/fake-news-classifier/main/app.py) -->
